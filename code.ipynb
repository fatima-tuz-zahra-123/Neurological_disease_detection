{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15af0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in testing folder: 84825\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "total_images = 0\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.png'):\n",
    "            total_images += 1\n",
    "\n",
    "print(\"Total images in testing folder:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1309767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33933561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD axial images: 17575\n",
      "CN axial images: 25795\n",
      "MCI axial images: 41455\n",
      "Total axial images: 84825\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "class_names = ['AD', 'CN', 'MCI']\n",
    "total_images = 0\n",
    "\n",
    "for class_name in class_names:\n",
    "    path = os.path.join(base_dir, class_name, 'axial')\n",
    "    if os.path.exists(path):\n",
    "        num_files = len([f for f in os.listdir(path) if f.lower().endswith('.png')])\n",
    "        print(f\"{class_name} axial images: {num_files}\")\n",
    "        total_images += num_files\n",
    "    else:\n",
    "        print(f\" Path not found: {path}\")\n",
    "\n",
    "print(\"Total axial images:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b440868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting CLIP Image features\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992716d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AD: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17575/17575 [18:58<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25795/25795 [3:27:01<00:00,  2.08it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MCI: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41455/41455 [55:53<00:00, 12.36it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors saved:\n",
      "train_features_axial.npy\n",
      "train_labels_axial.npy\n",
      "train_image_paths_axial.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug.augmenters as iaa\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import DenseNet121.  # densenet code\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Extracting CLIP Image features\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define your own compatible augmentations\n",
    "augmenter = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(10),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization\n",
    "])\n",
    "\n",
    "# EfficientNet base model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# # Step 3: Build feature extractor model (outputs 256-dim feature vectors)\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Step 4: Setup paths and label mapping\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "classes = ['AD', 'CN', 'MCI']\n",
    "label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "features, labels, image_paths = [], [], []\n",
    "\n",
    "# Step 5: Loop through dataset and extract features\n",
    "for cls in classes:\n",
    "    print(cls)\n",
    "    class_dir = os.path.join(base_dir, cls, 'axial')\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Directory not found: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    for fname in tqdm(os.listdir(class_dir), desc=f\"Processing {cls}\"):\n",
    "        if fname.lower().endswith('.png'):\n",
    "            img_path = os.path.join(class_dir, fname)\n",
    "            try:\n",
    "                pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                # --------- Original Image ---------\n",
    "                orig_img = preprocess_clip(pil_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    orig_feat = model.encode_image(orig_img).cpu().numpy().flatten()\n",
    "                features.append(orig_feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path)\n",
    "\n",
    "                # --------- Augmented Image ---------\n",
    "                aug_img_tensor = augmenter(pil_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    aug_feat = model.encode_image(aug_img_tensor).cpu().numpy().flatten()\n",
    "                features.append(aug_feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path + \"_aug\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {img_path}: {e}\")\n",
    "\n",
    "# Step 6: Save features, labels, and paths\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "np.save(\"train_features_axial.npy\", features)\n",
    "np.save(\"train_labels_axial.npy\", labels)\n",
    "np.save(\"train_image_paths_axial.npy\", image_paths)\n",
    "\n",
    "print(\"Feature vectors saved:\")\n",
    "print(\"train_features_axial.npy\")\n",
    "print(\"train_labels_axial.npy\")\n",
    "print(\"train_image_paths_axial.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ce3e5",
   "metadata": {},
   "source": [
    "same process as above for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb0718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AD: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1870/1870 [00:57<00:00, 32.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2585/2585 [01:20<00:00, 32.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MCI: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3960/3960 [02:03<00:00, 32.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors saved:\n",
      "val_features_axial.npy\n",
      "val_labels_axial.npy\n",
      "val_image_paths_axial.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug.augmenters as iaa\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import DenseNet121.  # densenet code\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "# Define your own compatible augmentations\n",
    "augmenter = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(10),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization\n",
    "])\n",
    "\n",
    "\n",
    "# EfficientNet ase model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# # Step 3: Build feature extractor model (outputs 256-dim feature vectors)\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Step 4: Setup paths and label mapping\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/val\"\n",
    "classes = ['AD', 'CN', 'MCI']\n",
    "label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "features, labels, image_paths = [], [], []\n",
    "\n",
    "# Step 5: Loop through dataset and extract features\n",
    "for cls in classes:\n",
    "    print(cls)\n",
    "    class_dir = os.path.join(base_dir, cls, 'axial')\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Directory not found: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    for fname in tqdm(os.listdir(class_dir), desc=f\"Processing {cls}\"):\n",
    "        if fname.lower().endswith('.png'):\n",
    "            img_path = os.path.join(class_dir, fname)\n",
    "            try:\n",
    "                img = preprocess_clip(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    feat = model.encode_image(img)\n",
    "                    feat = feat.cpu().numpy().flatten()\n",
    "                features.append(feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed: {img_path}, {e}\")\n",
    "\n",
    "# Step 6: Save features, labels, and paths\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "np.save(\"val_features_axial.npy\", features)\n",
    "np.save(\"val_labels_axial.npy\", labels)\n",
    "np.save(\"val_image_paths_axial.npy\", image_paths)\n",
    "\n",
    "print(\"Feature vectors saved:\")\n",
    "print(\"val_features_axial.npy\")\n",
    "print(\"val_labels_axial.npy\")\n",
    "print(\"val_image_paths_axial.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d7369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169650, 512)\n",
      "(169650,)\n",
      "(169650,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"train_features_axial.npy\")       # Shape: (84755, 256)\n",
    "print(features.shape)\n",
    "labels = np.load(\"train_labels_axial.npy\")           # Shape: (84755,)\n",
    "print(labels.shape)\n",
    "image_paths = np.load(\"train_image_paths_axial.npy\") # Shape: (84755,)\n",
    "print(image_paths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd24bc",
   "metadata": {},
   "source": [
    "loading and printing features and other things for validation data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b05433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8415, 512)\n",
      "(8415,)\n",
      "(8415,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"val_features_axial.npy\")       # Shape: (84755, 256)\n",
    "print(features.shape)\n",
    "labels = np.load(\"val_labels_axial.npy\")           # Shape: (84755,)\n",
    "print(labels.shape)\n",
    "image_paths = np.load(\"val_image_paths_axial.npy\") # Shape: (84755,)\n",
    "print(image_paths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17262be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: train_axial_features_and_labels_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the .npy files\n",
    "import pandas as pd\n",
    "features = np.load(\"train_features_axial.npy\")       # Shape: (84755, 256)\n",
    "labels = np.load(\"train_labels_axial.npy\")           # Shape: (84755,)\n",
    "\n",
    "# Combine features and labels\n",
    "combined = np.hstack((features, labels.reshape(-1, 1)))  # Shape: (84755, 257)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "\n",
    "# Optionally name columns\n",
    "\n",
    "feature_columns = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "df.columns = feature_columns + [\"label\"]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train_axial_features_and_labels_only.csv\", index=False)\n",
    "\n",
    "print(\" Saved: train_axial_features_and_labels_only.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8f8f1",
   "metadata": {},
   "source": [
    "same process for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af99964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: val_axial_features_and_labels_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the .npy files\n",
    "features = np.load(\"val_features_axial.npy\")       # Shape: (84755, 256)\n",
    "labels = np.load(\"val_labels_axial.npy\")           # Shape: (84755,)\n",
    "\n",
    "# Combine features and labels\n",
    "combined = np.hstack((features, labels.reshape(-1, 1)))  # Shape: (84755, 257)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "\n",
    "# Optionally name columns\n",
    "feature_columns = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "df.columns = feature_columns + [\"label\"]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"val_axial_features_and_labels_only.csv\", index=False)\n",
    "\n",
    "print(\" Saved: val_axial_features_and_labels_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526bf0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2294, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Data ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Acq Date</th>\n",
       "      <th>Format</th>\n",
       "      <th>Downloaded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I112538</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>m12</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>6/01/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I97341</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>m06</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/27/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I97327</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>sc</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>3/02/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I63874</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>sc</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>1/30/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I75150</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>m06</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>8/24/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image Data ID     Subject Group Sex  Age Visit Modality  \\\n",
       "0       I112538  941_S_1311   MCI   M   70   m12      MRI   \n",
       "1        I97341  941_S_1311   MCI   M   70   m06      MRI   \n",
       "2        I97327  941_S_1311   MCI   M   69    sc      MRI   \n",
       "3        I63874  941_S_1202    CN   M   78    sc      MRI   \n",
       "4        I75150  941_S_1202    CN   M   78   m06      MRI   \n",
       "\n",
       "                                  Description       Type   Acq Date Format  \\\n",
       "0    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  6/01/2008  NiFTI   \n",
       "1  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  9/27/2007  NiFTI   \n",
       "2    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  3/02/2007  NiFTI   \n",
       "3  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  1/30/2007  NiFTI   \n",
       "4    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  8/24/2007  NiFTI   \n",
       "\n",
       "   Downloaded  \n",
       "0  12/07/2024  \n",
       "1  12/07/2024  \n",
       "2  12/07/2024  \n",
       "3  12/07/2024  \n",
       "4  12/07/2024  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for text embeddings\n",
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_csv(\"/Users/fatimatuzzahra/Downloads/ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca69f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/Users/fatimatuzzahra/Downloads/ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# Remove label \n",
    "df_cleaned = df.drop(columns=[\"Group\", \"Downloaded\", \"Modality\", \"Type\", \"Format\"])\n",
    "\n",
    "# Keep Image ID separately for future mapping\n",
    "image_ids = df_cleaned[\"Image Data ID\"].values\n",
    "\n",
    "# Drop ID from the text encoding input\n",
    "text_only = df_cleaned.drop(columns=[\"Image Data ID\"])\n",
    "\n",
    "# Convert each row to string and generate sentence embeddings\n",
    "texts = text_only.astype(str).agg(\" \".join, axis=1).tolist()\n",
    "\n",
    "\n",
    "# Join text per row\n",
    "texts = text_only.astype(str).agg(\" \".join, axis=1).tolist()\n",
    "\n",
    "# Tokenize all text using CLIP\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "# Generate text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens).cpu().numpy()  # shape: (n_samples, 512)\n",
    "# Save with Image IDs\n",
    "import numpy as np\n",
    "np.save(\"text_embeddings_cleaned.npy\", text_embeddings)\n",
    "np.save(\"text_image_ids.npy\", image_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580be165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: text_embeddings_cleaned_with_ids.csv\n",
      " Shape: (2294, 513)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned text embeddings and associated image IDs\n",
    "text_embeddings = np.load(\"text_embeddings_cleaned.npy\")           # shape: (2294, 384)\n",
    "text_image_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)  # shape: (2294,)\n",
    "\n",
    "# Convert to DataFrame and add IDs as the first column\n",
    "df = pd.DataFrame(text_embeddings)\n",
    "df.insert(0, \"Image_ID\", text_image_ids)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"text_embeddings_cleaned_with_ids.csv\", index=False)\n",
    "\n",
    "print(\" Saved: text_embeddings_cleaned_with_ids.csv\")\n",
    "print(\" Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917722db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169650/169650 [00:00<00:00, 380178.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matched samples: 169650\n",
      " Final train fused shape: (169650, 1024)\n",
      " Labels shape: (169650,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load image feature data\n",
    "image_features = np.load(\"train_features_axial.npy\")       # (84755, 256)\n",
    "image_labels = np.load(\"train_labels_axial.npy\")           # (84755,)\n",
    "image_paths = np.load(\"train_image_paths_axial.npy\")       # (84755,)\n",
    "\n",
    "# Load cleaned textual embeddings and IDs\n",
    "textual_embeddings = np.load(\"text_embeddings_cleaned.npy\")   # (2294, 384)\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)      # (2294,)\n",
    "\n",
    "# Step 1: Build a lookup from Image ID â†’ text embedding\n",
    "text_lookup = {id_: emb for id_, emb in zip(text_ids, textual_embeddings)}\n",
    "\n",
    "# Step 2: Match each image with its text embedding (based on ID prefix)\n",
    "fused_features = []\n",
    "fused_labels = []\n",
    "matched_count = 0\n",
    "\n",
    "for i, (img_feat, label, path) in enumerate(tqdm(zip(image_features, image_labels, image_paths), total=len(image_paths))):\n",
    "    filename = os.path.basename(path)  # e.g., 'I31143_AD_axial_55.png'\n",
    "    img_id = filename.split('_')[0]             # 'I31143'\n",
    "\n",
    "    if img_id in text_lookup:\n",
    "        text_feat = text_lookup[img_id]\n",
    "        fused = np.concatenate([img_feat, text_feat])  # shape (640,)\n",
    "        fused_features.append(fused)\n",
    "        fused_labels.append(label)\n",
    "        matched_count += 1\n",
    "\n",
    "print(f\" Matched samples: {matched_count}\")\n",
    "\n",
    "# Convert to arrays and save\n",
    "fused_features = np.array(fused_features)\n",
    "fused_labels = np.array(fused_labels)\n",
    "\n",
    "np.save(\"train_fused_features_clean.npy\", fused_features)\n",
    "np.save(\"train_fused_labels_clean.npy\", fused_labels)\n",
    "\n",
    "print(\" Final train fused shape:\", fused_features.shape)\n",
    "print(\" Labels shape:\", fused_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05348e3",
   "metadata": {},
   "source": [
    "fusion for the validation set is done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "672a555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8415/8415 [00:00<00:00, 259763.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matched samples: 8415\n",
      " Final val fused shape: (8415, 1024)\n",
      " Labels shape: (8415,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load image feature data\n",
    "image_features = np.load(\"val_features_axial.npy\")       # (84755, 256)\n",
    "image_labels = np.load(\"val_labels_axial.npy\")           # (84755,)\n",
    "image_paths = np.load(\"val_image_paths_axial.npy\")       # (84755,)\n",
    "\n",
    "# Load cleaned textual embeddings and IDs\n",
    "textual_embeddings = np.load(\"text_embeddings_cleaned.npy\")   # (2294, 384)\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)      # (2294,)\n",
    "\n",
    "# Step 1: Build a lookup from Image ID â†’ text embedding\n",
    "text_lookup = {id_: emb for id_, emb in zip(text_ids, textual_embeddings)}\n",
    "\n",
    "# Step 2: Match each image with its text embedding (based on ID prefix)\n",
    "fused_features = []\n",
    "fused_labels = []\n",
    "matched_count = 0\n",
    "\n",
    "for i, (img_feat, label, path) in enumerate(tqdm(zip(image_features, image_labels, image_paths), total=len(image_paths))):\n",
    "    filename = os.path.basename(path)  # e.g., 'I31143_AD_axial_55.png'\n",
    "    img_id = filename.split('_')[0]             # 'I31143'\n",
    "\n",
    "    if img_id in text_lookup:\n",
    "        text_feat = text_lookup[img_id]\n",
    "        fused = np.concatenate([img_feat, text_feat])  # shape (640,)\n",
    "        fused_features.append(fused)\n",
    "        fused_labels.append(label)\n",
    "        matched_count += 1\n",
    "\n",
    "print(f\" Matched samples: {matched_count}\")\n",
    "\n",
    "# Convert to arrays and save\n",
    "fused_features = np.array(fused_features)\n",
    "fused_labels = np.array(fused_labels)\n",
    "\n",
    "np.save(\"val_fused_features_clean.npy\", fused_features)\n",
    "np.save(\"val_fused_labels_clean.npy\", fused_labels)\n",
    "\n",
    "print(\" Final val fused shape:\", fused_features.shape)\n",
    "print(\" Labels shape:\", fused_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ac337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "X = np.load(\"train_fused_features_clean.npy\")\n",
    "y = np.load(\"train_fused_labels_clean.npy\")\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train_fused_embeddings_with_labels.csv\", index=False)\n",
    "print(\" Saved: train_fused_embeddings_with_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c2db0",
   "metadata": {},
   "source": [
    "savinf into csv the validation fused sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886ef189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: val_fused_embeddings_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "X = np.load(\"val_fused_features_clean.npy\")\n",
    "y = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"val_fused_embeddings_with_labels.csv\", index=False)\n",
    "print(\" Saved: val_fused_embeddings_with_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50158e89",
   "metadata": {},
   "source": [
    "saving image and textual data separately for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315c40f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved:\n",
      "  train_image_features_only.npy (shape: (169650, 256) )\n",
      "  train_text_features_only.npy  (shape: (169650, 768) )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load fused embeddings\n",
    "fused = np.load(\"train_fused_features_clean.npy\")  # (84755, 640)\n",
    "\n",
    "# Split features\n",
    "image_features = fused[:, :256]   # CNN-based\n",
    "text_features  = fused[:, 256:]   # Sentence-transformer-based\n",
    "\n",
    "# Save separately\n",
    "np.save(\"train_image_features_only.npy\", image_features)\n",
    "np.save(\"train_text_features_only.npy\", text_features)\n",
    "\n",
    "print(\" Saved:\")\n",
    "print(\"  train_image_features_only.npy (shape:\", image_features.shape, \")\")\n",
    "print(\"  train_text_features_only.npy  (shape:\", text_features.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136a005",
   "metadata": {},
   "source": [
    "saving image and textual data separately for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e6e17f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved:\n",
      "  val_image_features_only.npy (shape: (8415, 256) )\n",
      "  val_text_features_only.npy  (shape: (8415, 768) )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load fused embeddings\n",
    "fused = np.load(\"val_fused_features_clean.npy\")  # (84755, 640)\n",
    "\n",
    "# Split features\n",
    "image_features = fused[:, :256]   # CNN-based\n",
    "text_features  = fused[:, 256:]   # Sentence-transformer-based\n",
    "\n",
    "# Save separately\n",
    "np.save(\"val_image_features_only.npy\", image_features)\n",
    "np.save(\"val_text_features_only.npy\", text_features)\n",
    "\n",
    "print(\" Saved:\")\n",
    "print(\"  val_image_features_only.npy (shape:\", image_features.shape, \")\")\n",
    "print(\"  val_text_features_only.npy  (shape:\", text_features.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33f960",
   "metadata": {},
   "source": [
    "the code below is added just to save the progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f29456",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ðŸ”¹ Step 1: Load training set fused features and labels\n",
    "X_train = np.load(\"train_fused_features_clean.npy\")   # Shape: (84755, 640)\n",
    "y_train = np.load(\"train_fused_labels_clean.npy\")     # Shape: (84755,)\n",
    "\n",
    "# Step 2: Load validation set fused features and labels\n",
    "X_val = np.load(\"val_fused_features_clean.npy\")\n",
    "y_val = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# ðŸ”¹ Step 2: Train/test split (not used for testing file)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# ðŸ”¹ Step 3: Define and train MLP (no need of fitting again in test set)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256, 64),   # You can adjust the architecture\n",
    "    activation='relu',\n",
    "    learning_rate_init=0.0006,\n",
    "    solver='adam',\n",
    "    max_iter=50,          # Increase to 100â€“300 for better results if time allows\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ”¹ Step 4: Predict and evaluate\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n MLP Accuracy: {acc:.4f}\")\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"AD\", \"CN\", \"MCI\"]))\n",
    "\n",
    "# ðŸ”¹ Step 5: Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=[\"AD\", \"CN\", \"MCI\"], yticklabels=[\"AD\", \"CN\", \"MCI\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\" Confusion Matrix - MLP (Fused Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd979ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# --------------------- Define MLP Model ---------------------\n",
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 3)  # 3 classes: AD, CN, MCI\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1daffc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "# --------------------- Load Data ---------------------\n",
    "X_train = np.load(\"train_text_features_only.npy\")  # shape (N, 768)\n",
    "y_train = np.load(\"train_fused_labels_clean.npy\")\n",
    "X_val = np.load(\"val_text_features_only.npy\")\n",
    "y_val = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# --------------------- Normalize ---------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# --------------------- SMOTE Oversampling ---------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# --------------------- Tensor Conversion ---------------------\n",
    "X_train_tensor = torch.tensor(X_train_bal, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_bal, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "# --------------------- Training Setup ---------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MLPWithDropout().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "# --------------------- Train Loop ---------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------- Evaluate on Training Set ---------------------\n",
    "model.eval()\n",
    "y_train_preds = []\n",
    "y_train_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        y_train_preds.extend(preds.cpu().numpy())\n",
    "        y_train_true.extend(yb.numpy())\n",
    "\n",
    "train_acc = accuracy_score(y_train_true, y_train_preds)\n",
    "print(f\"ðŸ“Š Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# --------------------- Evaluate on Validation Set ---------------------\n",
    "y_val_preds = []\n",
    "y_val_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        y_val_preds.extend(preds.cpu().numpy())\n",
    "        y_val_true.extend(yb.numpy())\n",
    "\n",
    "val_acc = accuracy_score(y_val_true, y_val_preds)\n",
    "print(f\"âœ… Validation Accuracy: {val_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val_true, y_val_preds, target_names=[\"AD\", \"CN\", \"MCI\"]))\n",
    "\n",
    "# --------------------- Confusion Matrix ---------------------\n",
    "cm = confusion_matrix(y_val_true, y_val_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Purples\", xticklabels=[\"AD\", \"CN\", \"MCI\"], yticklabels=[\"AD\", \"CN\", \"MCI\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - MLP (Validation Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e51e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 0.6670\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.66      0.61      0.63      1870\n",
      "          CN       0.62      0.57      0.59      2585\n",
      "         MCI       0.69      0.76      0.72      3960\n",
      "\n",
      "    accuracy                           0.67      8415\n",
      "   macro avg       0.66      0.65      0.65      8415\n",
      "weighted avg       0.66      0.67      0.66      8415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming y_test and y_pred are already defined from your previous evaluation\n",
    "\n",
    "# Define class labels (you can update these if your label encoding is different)\n",
    "class_names = ['AD', 'CN', 'MCI']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val_true, y_val_preds)\n",
    "print(f\"\\n Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_val_true, y_val_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac21360",
   "metadata": {},
   "source": [
    "\t.   âœ… Replace np.concatenate() fusion with a trainable layer\n",
    "\tâ€¢\tâœ… Try BioCLIP or medical image-pretrained models\n",
    "\tâ€¢\tâœ… Improve tabular/text embedding via BioBERT or using key columns only\n",
    "\tâ€¢\tâœ… Regularize fusion with modality dropout\n",
    "\tâ€¢\tâœ… (Optional) Try late fusion/ensemble as a baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
