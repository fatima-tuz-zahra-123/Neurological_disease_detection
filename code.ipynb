{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15af0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in testing folder: 84825\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "total_images = 0\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.png'):\n",
    "            total_images += 1\n",
    "\n",
    "print(\"Total images in testing folder:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1309767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33933561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD axial images: 17575\n",
      "CN axial images: 25795\n",
      "MCI axial images: 41455\n",
      "Total axial images: 84825\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "class_names = ['AD', 'CN', 'MCI']\n",
    "total_images = 0\n",
    "\n",
    "for class_name in class_names:\n",
    "    path = os.path.join(base_dir, class_name, 'axial')\n",
    "    if os.path.exists(path):\n",
    "        num_files = len([f for f in os.listdir(path) if f.lower().endswith('.png')])\n",
    "        print(f\"{class_name} axial images: {num_files}\")\n",
    "        total_images += num_files\n",
    "    else:\n",
    "        print(f\" Path not found: {path}\")\n",
    "\n",
    "print(\"Total axial images:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4d13273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imgaug\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (3.8.4)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (0.23.2)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (4.11.0.86)\n",
      "Requirement already satisfied: imageio in /opt/anaconda3/lib/python3.12/site-packages (from imgaug) (2.33.1)\n",
      "Collecting Shapely (from imgaug)\n",
      "  Downloading shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug) (3.2.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->imgaug) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->imgaug) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->imgaug) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->imgaug) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->imgaug) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->imgaug) (2.9.0.post0)\n",
      "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Shapely, imgaug\n",
      "Successfully installed Shapely-2.1.1 imgaug-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992716d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AD: 100%|██████████| 17575/17575 [18:58<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CN: 100%|██████████| 25795/25795 [3:27:01<00:00,  2.08it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MCI: 100%|██████████| 41455/41455 [55:53<00:00, 12.36it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors saved:\n",
      "train_features_axial.npy\n",
      "train_labels_axial.npy\n",
      "train_image_paths_axial.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug.augmenters as iaa\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import DenseNet121.  # densenet code\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Extracting CLIP Image features\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define your own compatible augmentations\n",
    "augmenter = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(10),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization\n",
    "])\n",
    "\n",
    "# EfficientNet base model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# # Step 3: Build feature extractor model (outputs 256-dim feature vectors)\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Step 4: Setup paths and label mapping\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "classes = ['AD', 'CN', 'MCI']\n",
    "label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "features, labels, image_paths = [], [], []\n",
    "\n",
    "# Step 5: Loop through dataset and extract features\n",
    "for cls in classes:\n",
    "    print(cls)\n",
    "    class_dir = os.path.join(base_dir, cls, 'axial')\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Directory not found: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    for fname in tqdm(os.listdir(class_dir), desc=f\"Processing {cls}\"):\n",
    "        if fname.lower().endswith('.png'):\n",
    "            img_path = os.path.join(class_dir, fname)\n",
    "            try:\n",
    "                pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                # --------- Original Image ---------\n",
    "                orig_img = preprocess_clip(pil_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    orig_feat = model.encode_image(orig_img).cpu().numpy().flatten()\n",
    "                features.append(orig_feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path)\n",
    "\n",
    "                # --------- Augmented Image ---------\n",
    "                aug_img_tensor = augmenter(pil_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    aug_feat = model.encode_image(aug_img_tensor).cpu().numpy().flatten()\n",
    "                features.append(aug_feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path + \"_aug\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {img_path}: {e}\")\n",
    "\n",
    "# Step 6: Save features, labels, and paths\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "np.save(\"train_features_axial.npy\", features)\n",
    "np.save(\"train_labels_axial.npy\", labels)\n",
    "np.save(\"train_image_paths_axial.npy\", image_paths)\n",
    "\n",
    "print(\"Feature vectors saved:\")\n",
    "print(\"train_features_axial.npy\")\n",
    "print(\"train_labels_axial.npy\")\n",
    "print(\"train_image_paths_axial.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ce3e5",
   "metadata": {},
   "source": [
    "same process as above for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75cb0718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AD: 100%|██████████| 1870/1870 [00:57<00:00, 32.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CN: 100%|██████████| 2585/2585 [01:20<00:00, 32.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MCI: 100%|██████████| 3960/3960 [02:03<00:00, 32.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors saved:\n",
      "val_features_axial.npy\n",
      "val_labels_axial.npy\n",
      "val_image_paths_axial.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug.augmenters as iaa\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import DenseNet121.  # densenet code\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Extracting CLIP Image features\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define your own compatible augmentations\n",
    "augmenter = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(10),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization\n",
    "])\n",
    "\n",
    "\n",
    "# EfficientNet ase model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# # Step 3: Build feature extractor model (outputs 256-dim feature vectors)\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Step 4: Setup paths and label mapping\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/val\"\n",
    "classes = ['AD', 'CN', 'MCI']\n",
    "label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "features, labels, image_paths = [], [], []\n",
    "\n",
    "# Step 5: Loop through dataset and extract features\n",
    "for cls in classes:\n",
    "    print(cls)\n",
    "    class_dir = os.path.join(base_dir, cls, 'axial')\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Directory not found: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    for fname in tqdm(os.listdir(class_dir), desc=f\"Processing {cls}\"):\n",
    "        if fname.lower().endswith('.png'):\n",
    "            img_path = os.path.join(class_dir, fname)\n",
    "            try:\n",
    "                img = preprocess_clip(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    feat = model.encode_image(img)\n",
    "                    feat = feat.cpu().numpy().flatten()\n",
    "                features.append(feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed: {img_path}, {e}\")\n",
    "\n",
    "# Step 6: Save features, labels, and paths\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "np.save(\"val_features_axial.npy\", features)\n",
    "np.save(\"val_labels_axial.npy\", labels)\n",
    "np.save(\"val_image_paths_axial.npy\", image_paths)\n",
    "\n",
    "print(\"Feature vectors saved:\")\n",
    "print(\"val_features_axial.npy\")\n",
    "print(\"val_labels_axial.npy\")\n",
    "print(\"val_image_paths_axial.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2a5dcca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features_batches_axial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m counts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAD\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMCI\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m      6\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMCI\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(batch_dir)):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m         labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_dir, file))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features_batches_axial'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "batch_dir = \"features_batches_axial\"\n",
    "counts = {'AD': 0, 'CN': 0, 'MCI': 0}\n",
    "label_map = {0: 'AD', 1: 'CN', 2: 'MCI'}\n",
    "\n",
    "for file in sorted(os.listdir(batch_dir)):\n",
    "    if file.startswith(\"labels_\") and file.endswith(\".npy\"):\n",
    "        labels = np.load(os.path.join(batch_dir, file))\n",
    "        for label in labels:\n",
    "            class_name = label_map[int(label)]\n",
    "            counts[class_name] += 1\n",
    "\n",
    "print(\" Processed image counts per class:\")\n",
    "for cls in counts:\n",
    "    print(f\"{cls}: {counts[cls]} images extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a68cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ping storage.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e0a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing all feature batches\n",
    "batch_dir = \"features_batches_axial\"\n",
    "\n",
    "# Initialize full arrays\n",
    "all_features, all_labels, all_paths = [], [], []\n",
    "\n",
    "# Get sorted list of batch files (based on index number)\n",
    "batch_indices = sorted([\n",
    "    int(f.split(\"_\")[1].split(\".\")[0])\n",
    "    for f in os.listdir(batch_dir) if f.startswith(\"features_\")\n",
    "])\n",
    "\n",
    "print(f\" Merging {len(batch_indices)} batches...\")\n",
    "\n",
    "# Load and concatenate each batch\n",
    "for idx in batch_indices:\n",
    "    f_feat = os.path.join(batch_dir, f\"features_{idx}.npy\")\n",
    "    f_lbls = os.path.join(batch_dir, f\"labels_{idx}.npy\")\n",
    "    f_paths = os.path.join(batch_dir, f\"paths_{idx}.npy\")\n",
    "\n",
    "    features = np.load(f_feat)\n",
    "    labels = np.load(f_lbls)\n",
    "    paths = np.load(f_paths)\n",
    "\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "    all_paths.append(paths)\n",
    "\n",
    "#  Stack everything\n",
    "all_features = np.vstack(all_features)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_paths = np.concatenate(all_paths)\n",
    "\n",
    "# Save final merged files\n",
    "np.save(\"train_features_axial.npy\", all_features)\n",
    "np.save(\"train_labels_axial.npy\", all_labels)\n",
    "np.save(\"train_image_paths_axial.npy\", all_paths)\n",
    "\n",
    "print(\" Merge complete!\")\n",
    "print(f\" Total feature vectors: {all_features.shape[0]}\")\n",
    "print(f\" Feature shape per image: {all_features.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d7369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169650, 512)\n",
      "(169650,)\n",
      "(169650,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"train_features_axial.npy\")       # Shape: (84755, 256)\n",
    "print(features.shape)\n",
    "labels = np.load(\"train_labels_axial.npy\")           # Shape: (84755,)\n",
    "print(labels.shape)\n",
    "image_paths = np.load(\"train_image_paths_axial.npy\") # Shape: (84755,)\n",
    "print(image_paths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd24bc",
   "metadata": {},
   "source": [
    "loading and printing features and other things for validation data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b05433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8415, 512)\n",
      "(8415,)\n",
      "(8415,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"val_features_axial.npy\")       # Shape: (84755, 256)\n",
    "print(features.shape)\n",
    "labels = np.load(\"val_labels_axial.npy\")           # Shape: (84755,)\n",
    "print(labels.shape)\n",
    "image_paths = np.load(\"val_image_paths_axial.npy\") # Shape: (84755,)\n",
    "print(image_paths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17262be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: train_axial_features_and_labels_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the .npy files\n",
    "import pandas as pd\n",
    "features = np.load(\"train_features_axial.npy\")       # Shape: (84755, 256)\n",
    "labels = np.load(\"train_labels_axial.npy\")           # Shape: (84755,)\n",
    "\n",
    "# Combine features and labels\n",
    "combined = np.hstack((features, labels.reshape(-1, 1)))  # Shape: (84755, 257)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "\n",
    "# Optionally name columns\n",
    "\n",
    "feature_columns = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "df.columns = feature_columns + [\"label\"]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train_axial_features_and_labels_only.csv\", index=False)\n",
    "\n",
    "print(\" Saved: train_axial_features_and_labels_only.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8f8f1",
   "metadata": {},
   "source": [
    "same process for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af99964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: val_axial_features_and_labels_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the .npy files\n",
    "features = np.load(\"val_features_axial.npy\")       # Shape: (84755, 256)\n",
    "labels = np.load(\"val_labels_axial.npy\")           # Shape: (84755,)\n",
    "\n",
    "# Combine features and labels\n",
    "combined = np.hstack((features, labels.reshape(-1, 1)))  # Shape: (84755, 257)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "\n",
    "# Optionally name columns\n",
    "feature_columns = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "df.columns = feature_columns + [\"label\"]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"val_axial_features_and_labels_only.csv\", index=False)\n",
    "\n",
    "print(\" Saved: val_axial_features_and_labels_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "526bf0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2294, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Data ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Acq Date</th>\n",
       "      <th>Format</th>\n",
       "      <th>Downloaded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I112538</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>m12</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>6/01/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I97341</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>m06</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/27/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I97327</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>sc</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>3/02/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I63874</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>sc</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>1/30/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I75150</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>m06</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>8/24/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image Data ID     Subject Group Sex  Age Visit Modality  \\\n",
       "0       I112538  941_S_1311   MCI   M   70   m12      MRI   \n",
       "1        I97341  941_S_1311   MCI   M   70   m06      MRI   \n",
       "2        I97327  941_S_1311   MCI   M   69    sc      MRI   \n",
       "3        I63874  941_S_1202    CN   M   78    sc      MRI   \n",
       "4        I75150  941_S_1202    CN   M   78   m06      MRI   \n",
       "\n",
       "                                  Description       Type   Acq Date Format  \\\n",
       "0    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  6/01/2008  NiFTI   \n",
       "1  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  9/27/2007  NiFTI   \n",
       "2    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  3/02/2007  NiFTI   \n",
       "3  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  1/30/2007  NiFTI   \n",
       "4    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  8/24/2007  NiFTI   \n",
       "\n",
       "   Downloaded  \n",
       "0  12/07/2024  \n",
       "1  12/07/2024  \n",
       "2  12/07/2024  \n",
       "3  12/07/2024  \n",
       "4  12/07/2024  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for text embeddings\n",
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_csv(\"/Users/fatimatuzzahra/Downloads/ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# Remove label \n",
    "df_cleaned = df.drop(columns=[\"Group\", \"Downloaded\", \"Modality\", \"Type\", \"Format\"])\n",
    "\n",
    "# Keep Image ID separately for future mapping\n",
    "image_ids = df_cleaned[\"Image Data ID\"].values\n",
    "\n",
    "# Drop ID from the text encoding input\n",
    "text_only = df_cleaned.drop(columns=[\"Image Data ID\"])\n",
    "\n",
    "# Convert each row to string and generate sentence embeddings\n",
    "texts = text_only.astype(str).agg(\" \".join, axis=1).tolist()\n",
    "\n",
    "\n",
    "# Join text per row\n",
    "texts = text_only.astype(str).agg(\" \".join, axis=1).tolist()\n",
    "\n",
    "# Tokenize all text using CLIP\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "# Generate text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens).cpu().numpy()  # shape: (n_samples, 512)\n",
    "# Save with Image IDs\n",
    "import numpy as np\n",
    "np.save(\"text_embeddings_cleaned.npy\", text_embeddings)\n",
    "np.save(\"text_image_ids.npy\", image_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "580be165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: text_embeddings_cleaned_with_ids.csv\n",
      " Shape: (2294, 513)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned text embeddings and associated image IDs\n",
    "text_embeddings = np.load(\"text_embeddings_cleaned.npy\")           # shape: (2294, 384)\n",
    "text_image_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)  # shape: (2294,)\n",
    "\n",
    "# Convert to DataFrame and add IDs as the first column\n",
    "df = pd.DataFrame(text_embeddings)\n",
    "df.insert(0, \"Image_ID\", text_image_ids)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"text_embeddings_cleaned_with_ids.csv\", index=False)\n",
    "\n",
    "print(\" Saved: text_embeddings_cleaned_with_ids.csv\")\n",
    "print(\" Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "917722db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169650/169650 [00:00<00:00, 408388.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matched samples: 169650\n",
      " Final train fused shape: (169650, 1024)\n",
      " Labels shape: (169650,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load image feature data\n",
    "image_features = np.load(\"train_features_axial.npy\")       # (84755, 256)\n",
    "image_labels = np.load(\"train_labels_axial.npy\")           # (84755,)\n",
    "image_paths = np.load(\"train_image_paths_axial.npy\")       # (84755,)\n",
    "\n",
    "# Load cleaned textual embeddings and IDs\n",
    "textual_embeddings = np.load(\"text_embeddings_cleaned.npy\")   # (2294, 384)\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)      # (2294,)\n",
    "\n",
    "# Step 1: Build a lookup from Image ID → text embedding\n",
    "text_lookup = {id_: emb for id_, emb in zip(text_ids, textual_embeddings)}\n",
    "\n",
    "# Step 2: Match each image with its text embedding (based on ID prefix)\n",
    "fused_features = []\n",
    "fused_labels = []\n",
    "matched_count = 0\n",
    "\n",
    "for i, (img_feat, label, path) in enumerate(tqdm(zip(image_features, image_labels, image_paths), total=len(image_paths))):\n",
    "    filename = os.path.basename(path)  # e.g., 'I31143_AD_axial_55.png'\n",
    "    img_id = filename.split('_')[0]             # 'I31143'\n",
    "\n",
    "    if img_id in text_lookup:\n",
    "        text_feat = text_lookup[img_id]\n",
    "        fused = np.concatenate([img_feat, text_feat])  # shape (640,)\n",
    "        fused_features.append(fused)\n",
    "        fused_labels.append(label)\n",
    "        matched_count += 1\n",
    "\n",
    "print(f\" Matched samples: {matched_count}\")\n",
    "\n",
    "# Convert to arrays and save\n",
    "fused_features = np.array(fused_features)\n",
    "fused_labels = np.array(fused_labels)\n",
    "\n",
    "np.save(\"train_fused_features_clean.npy\", fused_features)\n",
    "np.save(\"train_fused_labels_clean.npy\", fused_labels)\n",
    "\n",
    "print(\" Final train fused shape:\", fused_features.shape)\n",
    "print(\" Labels shape:\", fused_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05348e3",
   "metadata": {},
   "source": [
    "fusion for the validation set is done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672a555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8415/8415 [00:00<00:00, 254223.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matched samples: 8415\n",
      " Final val fused shape: (8415, 1024)\n",
      " Labels shape: (8415,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load image feature data\n",
    "image_features = np.load(\"val_features_axial.npy\")       # (84755, 256)\n",
    "image_labels = np.load(\"val_labels_axial.npy\")           # (84755,)\n",
    "image_paths = np.load(\"val_image_paths_axial.npy\")       # (84755,)\n",
    "\n",
    "# Load cleaned textual embeddings and IDs\n",
    "textual_embeddings = np.load(\"text_embeddings_cleaned.npy\")   # (2294, 384)\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)      # (2294,)\n",
    "\n",
    "# Step 1: Build a lookup from Image ID → text embedding\n",
    "text_lookup = {id_: emb for id_, emb in zip(text_ids, textual_embeddings)}\n",
    "\n",
    "# Step 2: Match each image with its text embedding (based on ID prefix)\n",
    "fused_features = []\n",
    "fused_labels = []\n",
    "matched_count = 0\n",
    "\n",
    "for i, (img_feat, label, path) in enumerate(tqdm(zip(image_features, image_labels, image_paths), total=len(image_paths))):\n",
    "    filename = os.path.basename(path)  # e.g., 'I31143_AD_axial_55.png'\n",
    "    img_id = filename.split('_')[0]             # 'I31143'\n",
    "\n",
    "    if img_id in text_lookup:\n",
    "        text_feat = text_lookup[img_id]\n",
    "        fused = np.concatenate([img_feat, text_feat])  # shape (640,)\n",
    "        fused_features.append(fused)\n",
    "        fused_labels.append(label)\n",
    "        matched_count += 1\n",
    "\n",
    "print(f\" Matched samples: {matched_count}\")\n",
    "\n",
    "# Convert to arrays and save\n",
    "fused_features = np.array(fused_features)\n",
    "fused_labels = np.array(fused_labels)\n",
    "\n",
    "np.save(\"val_fused_features_clean.npy\", fused_features)\n",
    "np.save(\"val_fused_labels_clean.npy\", fused_labels)\n",
    "\n",
    "print(\" Final val fused shape:\", fused_features.shape)\n",
    "print(\" Labels shape:\", fused_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b11ac337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: train_fused_embeddings_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "X = np.load(\"train_fused_features_clean.npy\")\n",
    "y = np.load(\"train_fused_labels_clean.npy\")\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train_fused_embeddings_with_labels.csv\", index=False)\n",
    "print(\" Saved: train_fused_embeddings_with_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c2db0",
   "metadata": {},
   "source": [
    "savinf into csv the validation fused sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "886ef189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: val_fused_embeddings_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "X = np.load(\"val_fused_features_clean.npy\")\n",
    "y = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"val_fused_embeddings_with_labels.csv\", index=False)\n",
    "print(\" Saved: val_fused_embeddings_with_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50158e89",
   "metadata": {},
   "source": [
    "saving image and textual data separately for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "315c40f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved:\n",
      "  train_image_features_only.npy (shape: (169650, 256) )\n",
      "  train_text_features_only.npy  (shape: (169650, 768) )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load fused embeddings\n",
    "fused = np.load(\"train_fused_features_clean.npy\")  # (84755, 640)\n",
    "\n",
    "# Split features\n",
    "image_features = fused[:, :256]   # CNN-based\n",
    "text_features  = fused[:, 256:]   # Sentence-transformer-based\n",
    "\n",
    "# Save separately\n",
    "np.save(\"train_image_features_only.npy\", image_features)\n",
    "np.save(\"train_text_features_only.npy\", text_features)\n",
    "\n",
    "print(\" Saved:\")\n",
    "print(\"  train_image_features_only.npy (shape:\", image_features.shape, \")\")\n",
    "print(\"  train_text_features_only.npy  (shape:\", text_features.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136a005",
   "metadata": {},
   "source": [
    "saving image and textual data separately for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e6e17f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved:\n",
      "  val_image_features_only.npy (shape: (8415, 256) )\n",
      "  val_text_features_only.npy  (shape: (8415, 768) )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load fused embeddings\n",
    "fused = np.load(\"val_fused_features_clean.npy\")  # (84755, 640)\n",
    "\n",
    "# Split features\n",
    "image_features = fused[:, :256]   # CNN-based\n",
    "text_features  = fused[:, 256:]   # Sentence-transformer-based\n",
    "\n",
    "# Save separately\n",
    "np.save(\"val_image_features_only.npy\", image_features)\n",
    "np.save(\"val_text_features_only.npy\", text_features)\n",
    "\n",
    "print(\" Saved:\")\n",
    "print(\"  val_image_features_only.npy (shape:\", image_features.shape, \")\")\n",
    "print(\"  val_text_features_only.npy  (shape:\", text_features.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33f960",
   "metadata": {},
   "source": [
    "the code below is added just to save the progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f29456",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 🔹 Step 1: Load training set fused features and labels\n",
    "X_train = np.load(\"train_fused_features_clean.npy\")   # Shape: (84755, 640)\n",
    "y_train = np.load(\"train_fused_labels_clean.npy\")     # Shape: (84755,)\n",
    "\n",
    "# Step 2: Load validation set fused features and labels\n",
    "X_val = np.load(\"val_fused_features_clean.npy\")\n",
    "y_val = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# 🔹 Step 2: Train/test split (not used for testing file)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# 🔹 Step 3: Define and train MLP (no need of fitting again in test set)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256, 64),   # You can adjust the architecture\n",
    "    activation='relu',\n",
    "    learning_rate_init=0.0006,\n",
    "    solver='adam',\n",
    "    max_iter=50,          # Increase to 100–300 for better results if time allows\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Step 4: Predict and evaluate\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n MLP Accuracy: {acc:.4f}\")\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"AD\", \"CN\", \"MCI\"]))\n",
    "\n",
    "# 🔹 Step 5: Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=[\"AD\", \"CN\", \"MCI\"], yticklabels=[\"AD\", \"CN\", \"MCI\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\" Confusion Matrix - MLP (Fused Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "766f56d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(unique, counts)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd979ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1024, 512),   # input size must match your fused feature size (e.g., 1024 if CLIP image+text)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 3)       # 3 output classes (AD, CN, MCI)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1daffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5824\n",
      "Epoch 2/10, Loss: 0.1643\n",
      "Epoch 3/10, Loss: 0.0813\n",
      "Epoch 4/10, Loss: 0.0514\n",
      "Epoch 5/10, Loss: 0.0387\n",
      "Epoch 6/10, Loss: 0.0311\n",
      "Epoch 7/10, Loss: 0.0259\n",
      "Epoch 8/10, Loss: 0.0218\n",
      "Epoch 9/10, Loss: 0.0196\n",
      "Epoch 10/10, Loss: 0.0182\n",
      "📊 Training Accuracy: 1.0000\n",
      "✅ Validation Accuracy: 0.6671\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.71      0.57      0.63      1870\n",
      "          CN       0.62      0.61      0.61      2585\n",
      "         MCI       0.68      0.75      0.71      3960\n",
      "\n",
      "    accuracy                           0.67      8415\n",
      "   macro avg       0.67      0.64      0.65      8415\n",
      "weighted avg       0.67      0.67      0.67      8415\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "X_train = np.load(\"train_fused_features_clean.npy\")\n",
    "y_train = np.load(\"train_fused_labels_clean.npy\")\n",
    "X_val = np.load(\"val_fused_features_clean.npy\")\n",
    "y_val = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# Optional: if your MLP has 1024 input, pad image/text features accordingly\n",
    "# assert X_train.shape[1] == 1024\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_bal, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_bal, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "\n",
    "model = MLPWithDropout().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 🔹 Evaluate on Training Set\n",
    "model.eval()\n",
    "y_train_preds = []\n",
    "y_train_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        y_train_preds.extend(preds.cpu().numpy())\n",
    "        y_train_true.extend(yb.numpy())\n",
    "\n",
    "# Training Metrics\n",
    "train_acc = accuracy_score(y_train_true, y_train_preds)\n",
    "print(f\"📊 Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# 🔹 Evaluate on Validation Set\n",
    "model.eval()\n",
    "y_val_preds = []\n",
    "y_val_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        y_val_preds.extend(preds.cpu().numpy())\n",
    "        y_val_true.extend(yb.numpy())\n",
    "\n",
    "# Metrics\n",
    "val_acc = accuracy_score(y_val_true, y_val_preds)\n",
    "print(f\"✅ Validation Accuracy: {val_acc:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_true, y_val_preds, target_names=[\"AD\", \"CN\", \"MCI\"]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val_true, y_val_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Purples\", xticklabels=[\"AD\", \"CN\", \"MCI\"], yticklabels=[\"AD\", \"CN\", \"MCI\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - MLP (Validation Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e51e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 0.6765\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.71      0.57      0.63      1870\n",
      "          CN       0.64      0.60      0.62      2585\n",
      "         MCI       0.69      0.78      0.73      3960\n",
      "\n",
      "    accuracy                           0.68      8415\n",
      "   macro avg       0.68      0.65      0.66      8415\n",
      "weighted avg       0.68      0.68      0.67      8415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming y_test and y_pred are already defined from your previous evaluation\n",
    "\n",
    "# Define class labels (you can update these if your label encoding is different)\n",
    "class_names = ['AD', 'CN', 'MCI']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val_true, y_val_preds)\n",
    "print(f\"\\n Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_val_true, y_val_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517f460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
