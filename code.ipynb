{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15af0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in testing folder: 84825\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "total_images = 0\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.png'):\n",
    "            total_images += 1\n",
    "\n",
    "print(\"Total images in testing folder:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1309767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33933561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD axial images: 17575\n",
      "CN axial images: 25795\n",
      "MCI axial images: 41455\n",
      "Total axial images: 84825\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "class_names = ['AD', 'CN', 'MCI']\n",
    "total_images = 0\n",
    "\n",
    "for class_name in class_names:\n",
    "    path = os.path.join(base_dir, class_name, 'axial')\n",
    "    if os.path.exists(path):\n",
    "        num_files = len([f for f in os.listdir(path) if f.lower().endswith('.png')])\n",
    "        print(f\"{class_name} axial images: {num_files}\")\n",
    "        total_images += num_files\n",
    "    else:\n",
    "        print(f\" Path not found: {path}\")\n",
    "\n",
    "print(\"Total axial images:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b440868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting CLIP Image features\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992716d1",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug.augmenters as iaa\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import DenseNet121.  # densenet code\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Extracting CLIP Image features\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define your own compatible augmentations\n",
    "augmenter = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(10),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization\n",
    "])\n",
    "\n",
    "# EfficientNet base model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# # Step 3: Build feature extractor model (outputs 256-dim feature vectors)\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Step 4: Setup paths and label mapping\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/train\"\n",
    "classes = ['AD', 'CN', 'MCI']\n",
    "label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "features, labels, image_paths = [], [], []\n",
    "\n",
    "# Step 5: Loop through dataset and extract features\n",
    "for cls in classes:\n",
    "    print(cls)\n",
    "    class_dir = os.path.join(base_dir, cls, 'axial')\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Directory not found: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    for fname in tqdm(os.listdir(class_dir), desc=f\"Processing {cls}\"):\n",
    "        if fname.lower().endswith('.png'):\n",
    "            img_path = os.path.join(class_dir, fname)\n",
    "            try:\n",
    "                pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                # --------- Original Image ---------\n",
    "                orig_img = preprocess_clip(pil_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    orig_feat = model.encode_image(orig_img).cpu().numpy().flatten()\n",
    "                features.append(orig_feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path)\n",
    "\n",
    "                # --------- Augmented Image ---------\n",
    "                aug_img_tensor = augmenter(pil_img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    aug_feat = model.encode_image(aug_img_tensor).cpu().numpy().flatten()\n",
    "                features.append(aug_feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path + \"_aug\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {img_path}: {e}\")\n",
    "\n",
    "# Step 6: Save features, labels, and paths\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "np.save(\"train_features_axial.npy\", features)\n",
    "np.save(\"train_labels_axial.npy\", labels)\n",
    "np.save(\"train_image_paths_axial.npy\", image_paths)\n",
    "\n",
    "print(\"Feature vectors saved:\")\n",
    "print(\"train_features_axial.npy\")\n",
    "print(\"train_labels_axial.npy\")\n",
    "print(\"train_image_paths_axial.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ce3e5",
   "metadata": {},
   "source": [
    "same process as above for validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb0718",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug.augmenters as iaa\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import DenseNet121.  # densenet code\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "# Define your own compatible augmentations\n",
    "augmenter = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(10),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization\n",
    "])\n",
    "\n",
    "\n",
    "# EfficientNet ase model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# # Step 3: Build feature extractor model (outputs 256-dim feature vectors)\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Step 4: Setup paths and label mapping\n",
    "base_dir = r\"/Users/fatimatuzzahra/Downloads/processed_slices/val\"\n",
    "classes = ['AD', 'CN', 'MCI']\n",
    "label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "features, labels, image_paths = [], [], []\n",
    "\n",
    "# Step 5: Loop through dataset and extract features\n",
    "for cls in classes:\n",
    "    print(cls)\n",
    "    class_dir = os.path.join(base_dir, cls, 'axial')\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Directory not found: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    for fname in tqdm(os.listdir(class_dir), desc=f\"Processing {cls}\"):\n",
    "        if fname.lower().endswith('.png'):\n",
    "            img_path = os.path.join(class_dir, fname)\n",
    "            try:\n",
    "                img = preprocess_clip(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    feat = model.encode_image(img)\n",
    "                    feat = feat.cpu().numpy().flatten()\n",
    "                features.append(feat)\n",
    "                labels.append(label_map[cls])\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed: {img_path}, {e}\")\n",
    "\n",
    "# Step 6: Save features, labels, and paths\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "np.save(\"val_features_axial.npy\", features)\n",
    "np.save(\"val_labels_axial.npy\", labels)\n",
    "np.save(\"val_image_paths_axial.npy\", image_paths)\n",
    "\n",
    "print(\"Feature vectors saved:\")\n",
    "print(\"val_features_axial.npy\")\n",
    "print(\"val_labels_axial.npy\")\n",
    "print(\"val_image_paths_axial.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7369c",
   "metadata": {},
   "source": [
    "features = np.load(\"train_features_axial.npy\")       # Shape: (84755, 256)\n",
    "print(features.shape)\n",
    "labels = np.load(\"train_labels_axial.npy\")           # Shape: (84755,)\n",
    "print(labels.shape)\n",
    "image_paths = np.load(\"train_image_paths_axial.npy\") # Shape: (84755,)\n",
    "print(image_paths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd24bc",
   "metadata": {},
   "source": [
    "loading and printing features and other things for validation data as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b05433",
   "metadata": {},
   "source": [
    "features = np.load(\"val_features_axial.npy\")       # Shape: (84755, 256)\n",
    "print(features.shape)\n",
    "labels = np.load(\"val_labels_axial.npy\")           # Shape: (84755,)\n",
    "print(labels.shape)\n",
    "image_paths = np.load(\"val_image_paths_axial.npy\") # Shape: (84755,)\n",
    "print(image_paths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17262be6",
   "metadata": {},
   "source": [
    "# Load the .npy files\n",
    "import pandas as pd\n",
    "features = np.load(\"train_features_axial.npy\")       # Shape: (84755, 256)\n",
    "labels = np.load(\"train_labels_axial.npy\")           # Shape: (84755,)\n",
    "\n",
    "# Combine features and labels\n",
    "combined = np.hstack((features, labels.reshape(-1, 1)))  # Shape: (84755, 257)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "\n",
    "# Optionally name columns\n",
    "\n",
    "feature_columns = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "df.columns = feature_columns + [\"label\"]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train_axial_features_and_labels_only.csv\", index=False)\n",
    "\n",
    "print(\" Saved: train_axial_features_and_labels_only.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8f8f1",
   "metadata": {},
   "source": [
    "same process for validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af99964",
   "metadata": {},
   "source": [
    "# Load the .npy files\n",
    "features = np.load(\"val_features_axial.npy\")       # Shape: (84755, 256)\n",
    "labels = np.load(\"val_labels_axial.npy\")           # Shape: (84755,)\n",
    "\n",
    "# Combine features and labels\n",
    "combined = np.hstack((features, labels.reshape(-1, 1)))  # Shape: (84755, 257)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "\n",
    "# Optionally name columns\n",
    "feature_columns = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "df.columns = feature_columns + [\"label\"]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"val_axial_features_and_labels_only.csv\", index=False)\n",
    "\n",
    "print(\" Saved: val_axial_features_and_labels_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2c209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526bf0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2294, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Data ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Acq Date</th>\n",
       "      <th>Format</th>\n",
       "      <th>Downloaded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I112538</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>m12</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>6/01/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I97341</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>m06</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/27/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I97327</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>sc</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>3/02/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I63874</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>sc</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>1/30/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I75150</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>m06</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>8/24/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>12/07/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image Data ID     Subject Group Sex  Age Visit Modality  \\\n",
       "0       I112538  941_S_1311   MCI   M   70   m12      MRI   \n",
       "1        I97341  941_S_1311   MCI   M   70   m06      MRI   \n",
       "2        I97327  941_S_1311   MCI   M   69    sc      MRI   \n",
       "3        I63874  941_S_1202    CN   M   78    sc      MRI   \n",
       "4        I75150  941_S_1202    CN   M   78   m06      MRI   \n",
       "\n",
       "                                  Description       Type   Acq Date Format  \\\n",
       "0    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  6/01/2008  NiFTI   \n",
       "1  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  9/27/2007  NiFTI   \n",
       "2    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  3/02/2007  NiFTI   \n",
       "3  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  1/30/2007  NiFTI   \n",
       "4    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  8/24/2007  NiFTI   \n",
       "\n",
       "   Downloaded  \n",
       "0  12/07/2024  \n",
       "1  12/07/2024  \n",
       "2  12/07/2024  \n",
       "3  12/07/2024  \n",
       "4  12/07/2024  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for text embeddings\n",
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_csv(\"/Users/fatimatuzzahra/Downloads/ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca69f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - train_text_features_only.npy (2294, 512)\n",
      " - train_text_labels_only.npy (2294,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/Users/fatimatuzzahra/Downloads/ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# üîπ Step 1: Keep label and encode it numerically\n",
    "label_map = {'AD': 0, 'CN': 1, 'MCI': 2}\n",
    "df = df[df['Group'].isin(label_map)]  # filter out unknowns\n",
    "df['label'] = df['Group'].map(label_map)\n",
    "\n",
    "# üîπ Step 2: Prepare text input (excluding label columns)\n",
    "text_input_cols = [col for col in df.columns if col not in ['Group', 'label', 'Downloaded', 'Modality', 'Type', 'Format']]\n",
    "df_text = df[text_input_cols]\n",
    "\n",
    "# üîπ Optional: Keep Image IDs if you want to track\n",
    "image_ids = df['Image Data ID'].values\n",
    "\n",
    "# üîπ Step 3: Combine row text into string per patient\n",
    "texts = df_text.astype(str).agg(\" \".join, axis=1).tolist()\n",
    "\n",
    "# üîπ Step 4: Tokenize with CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "# üîπ Step 5: Generate text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens).cpu().numpy()\n",
    "\n",
    "# üîπ Step 6: Save embeddings and labels (ready for training!)\n",
    "np.save(\"train_text_features_only.npy\", text_embeddings)\n",
    "np.save(\"train_text_labels_only.npy\", df['label'].values)\n",
    "\n",
    "# Optional: save a CSV for review\n",
    "df_out = pd.DataFrame(text_embeddings)\n",
    "df_out['label'] = df['label'].values\n",
    "df_out.to_csv(\"train_text_features_with_labels.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - train_text_features_only.npy\", text_embeddings.shape)\n",
    "print(\" - train_text_labels_only.npy\", df['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833bfba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2294, 513)\n",
      "Final dataframe shape: (2294, 513)\n",
      "All column names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', 'label']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_text_features_with_labels.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "print(\"Final dataframe shape:\", df.shape)\n",
    "print(\"All column names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae66e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done!\n",
      "Train: (1541, 512)\n",
      "Val: (153, 512)\n",
      "Test: (226, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load image paths (original splits)\n",
    "train_paths = np.load(\"train_image_paths_axial.npy\", allow_pickle=True)\n",
    "val_paths = np.load(\"val_image_paths_axial.npy\", allow_pickle=True)\n",
    "test_paths = np.load(\"test_image_paths_axial.npy\", allow_pickle=True)\n",
    "\n",
    "# Extract image IDs from filenames (e.g., \"I12345_AD_axial_45.png\" -> \"I12345\")\n",
    "def extract_ids(paths):\n",
    "    return set(os.path.basename(p).split(\"_\")[0] for p in paths)\n",
    "\n",
    "train_ids = extract_ids(train_paths)\n",
    "val_ids = extract_ids(val_paths)\n",
    "test_ids = extract_ids(test_paths)\n",
    "\n",
    "# Load textual data\n",
    "text_features = np.load(\"train_text_features_only.npy\")  # shape: (2294, D)\n",
    "text_labels = np.load(\"train_text_labels_only.npy\")\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)\n",
    "\n",
    "# Match and split text data\n",
    "train_feats, val_feats, test_feats = [], [], []\n",
    "train_labs, val_labs, test_labs = [], [], []\n",
    "\n",
    "for feat, label, tid in zip(text_features, text_labels, text_ids):\n",
    "    if tid in train_ids:\n",
    "        train_feats.append(feat)\n",
    "        train_labs.append(label)\n",
    "    elif tid in val_ids:\n",
    "        val_feats.append(feat)\n",
    "        val_labs.append(label)\n",
    "    elif tid in test_ids:\n",
    "        test_feats.append(feat)\n",
    "        test_labs.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_feats, val_feats, test_feats = map(np.array, [train_feats, val_feats, test_feats])\n",
    "train_labs, val_labs, test_labs = map(np.array, [train_labs, val_labs, test_labs])\n",
    "\n",
    "# Save\n",
    "np.save(\"text_train_features.npy\", train_feats)\n",
    "np.save(\"text_train_labels.npy\", train_labs)\n",
    "np.save(\"text_val_features.npy\", val_feats)\n",
    "np.save(\"text_val_labels.npy\", val_labs)\n",
    "np.save(\"text_test_features.npy\", test_feats)\n",
    "np.save(\"text_test_labels.npy\", test_labs)\n",
    "\n",
    "print(\"‚úÖ Done!\")\n",
    "print(\"Train:\", train_feats.shape)\n",
    "print(\"Val:\", val_feats.shape)\n",
    "print(\"Test:\", test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430fa698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for ID overlaps between splits:\n",
      "Train ‚à© Val: 0\n",
      "Train ‚à© Test: 0\n",
      "Val ‚à© Test: 0\n",
      "\n",
      "üì¶ Text sample distribution:\n",
      "Text IDs in Train: 1541\n",
      "Text IDs in Val: 153\n",
      "Text IDs in Test: 226\n",
      "\n",
      "üîÅ Overlap in Textual Data:\n",
      "Text Train ‚à© Val: 0\n",
      "Text Train ‚à© Test: 0\n",
      "Text Val ‚à© Test: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load original image paths\n",
    "train_paths = np.load(\"train_image_paths_axial.npy\", allow_pickle=True)\n",
    "val_paths = np.load(\"val_image_paths_axial.npy\", allow_pickle=True)\n",
    "test_paths = np.load(\"test_image_paths_axial.npy\", allow_pickle=True)\n",
    "\n",
    "# Extract image IDs (e.g., \"I12345_AD_axial_55.png\" ‚Üí \"I12345\")\n",
    "def extract_ids(paths):\n",
    "    return set(os.path.basename(p).split(\"_\")[0] for p in paths)\n",
    "\n",
    "train_ids = extract_ids(train_paths)\n",
    "val_ids = extract_ids(val_paths)\n",
    "test_ids = extract_ids(test_paths)\n",
    "\n",
    "# 1. Check for overlaps between splits\n",
    "overlap_train_val = train_ids & val_ids\n",
    "overlap_train_test = train_ids & test_ids\n",
    "overlap_val_test = val_ids & test_ids\n",
    "\n",
    "# Print results\n",
    "print(\"üîç Checking for ID overlaps between splits:\")\n",
    "print(\"Train ‚à© Val:\", len(overlap_train_val))\n",
    "print(\"Train ‚à© Test:\", len(overlap_train_test))\n",
    "print(\"Val ‚à© Test:\", len(overlap_val_test))\n",
    "\n",
    "# 2. (Optional) Also check if any text_image_ids are used in multiple splits\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)\n",
    "text_ids_set = set(text_ids)\n",
    "\n",
    "text_in_train = text_ids_set & train_ids\n",
    "text_in_val = text_ids_set & val_ids\n",
    "text_in_test = text_ids_set & test_ids\n",
    "\n",
    "print(\"\\nüì¶ Text sample distribution:\")\n",
    "print(\"Text IDs in Train:\", len(text_in_train))\n",
    "print(\"Text IDs in Val:\", len(text_in_val))\n",
    "print(\"Text IDs in Test:\", len(text_in_test))\n",
    "\n",
    "# Sanity check for overlaps in textual split\n",
    "print(\"\\nüîÅ Overlap in Textual Data:\")\n",
    "print(\"Text Train ‚à© Val:\", len(text_in_train & text_in_val))\n",
    "print(\"Text Train ‚à© Test:\", len(text_in_train & text_in_test))\n",
    "print(\"Text Val ‚à© Test:\", len(text_in_val & text_in_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580be165",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned text embeddings and associated image IDs\n",
    "text_embeddings = np.load(\"text_embeddings_cleaned.npy\")           # shape: (2294, 384)\n",
    "text_image_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)  # shape: (2294,)\n",
    "\n",
    "# Convert to DataFrame and add IDs as the first column\n",
    "df = pd.DataFrame(text_embeddings)\n",
    "df.insert(0, \"Image_ID\", text_image_ids)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"text_embeddings_cleaned_with_ids.csv\", index=False)\n",
    "\n",
    "print(\" Saved: text_embeddings_cleaned_with_ids.csv\")\n",
    "print(\" Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917722db",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load image feature data\n",
    "image_features = np.load(\"train_features_axial.npy\")       # (84755, 256)\n",
    "image_labels = np.load(\"train_labels_axial.npy\")           # (84755,)\n",
    "image_paths = np.load(\"train_image_paths_axial.npy\")       # (84755,)\n",
    "\n",
    "# Load cleaned textual embeddings and IDs\n",
    "textual_embeddings = np.load(\"text_embeddings_cleaned.npy\")   # (2294, 384)\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)      # (2294,)\n",
    "\n",
    "# Step 1: Build a lookup from Image ID ‚Üí text embedding\n",
    "text_lookup = {id_: emb for id_, emb in zip(text_ids, textual_embeddings)}\n",
    "\n",
    "# Step 2: Match each image with its text embedding (based on ID prefix)\n",
    "fused_features = []\n",
    "fused_labels = []\n",
    "matched_count = 0\n",
    "\n",
    "for i, (img_feat, label, path) in enumerate(tqdm(zip(image_features, image_labels, image_paths), total=len(image_paths))):\n",
    "    filename = os.path.basename(path)  # e.g., 'I31143_AD_axial_55.png'\n",
    "    img_id = filename.split('_')[0]             # 'I31143'\n",
    "\n",
    "    if img_id in text_lookup:\n",
    "        text_feat = text_lookup[img_id]\n",
    "        fused = np.concatenate([img_feat, text_feat])  # shape (640,)\n",
    "        fused_features.append(fused)\n",
    "        fused_labels.append(label)\n",
    "        matched_count += 1\n",
    "        \n",
    "\n",
    "print(f\" Matched samples: {matched_count}\")\n",
    "\n",
    "# Convert to arrays and save\n",
    "fused_features = np.array(fused_features)\n",
    "fused_labels = np.array(fused_labels)\n",
    "\n",
    "np.save(\"train_fused_features_clean.npy\", fused_features)\n",
    "np.save(\"train_fused_labels_clean.npy\", fused_labels)\n",
    "\n",
    "print(\" Final train fused shape:\", fused_features.shape)\n",
    "print(\" Labels shape:\", fused_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199516b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched text IDs: 1541\n",
      "‚ùå Unmatched text IDs: 753\n",
      "Total text IDs: 2294\n"
     ]
    }
   ],
   "source": [
    "# Extract image IDs from image paths\n",
    "image_ids = set(os.path.basename(p).split('_')[0] for p in image_paths)\n",
    "\n",
    "# Convert text_ids to set\n",
    "text_ids_set = set(text_ids)\n",
    "\n",
    "# Compute how many text IDs have a matching image ID\n",
    "matching_ids = text_ids_set & image_ids\n",
    "print(\"‚úÖ Matched text IDs:\", len(matching_ids))\n",
    "print(\"‚ùå Unmatched text IDs:\", len(text_ids_set - image_ids))\n",
    "print(\"Total text IDs:\", len(text_ids_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05348e3",
   "metadata": {},
   "source": [
    "fusion for the validation set is done below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a555e",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load image feature data\n",
    "image_features = np.load(\"val_features_axial.npy\")       # (84755, 256)\n",
    "image_labels = np.load(\"val_labels_axial.npy\")           # (84755,)\n",
    "image_paths = np.load(\"val_image_paths_axial.npy\")       # (84755,)\n",
    "\n",
    "# Load cleaned textual embeddings and IDs\n",
    "textual_embeddings = np.load(\"text_embeddings_cleaned.npy\")   # (2294, 384)\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)      # (2294,)\n",
    "\n",
    "# Step 1: Build a lookup from Image ID ‚Üí text embedding\n",
    "text_lookup = {id_: emb for id_, emb in zip(text_ids, textual_embeddings)}\n",
    "\n",
    "# Step 2: Match each image with its text embedding (based on ID prefix)\n",
    "fused_features = []\n",
    "fused_labels = []\n",
    "matched_count = 0\n",
    "\n",
    "for i, (img_feat, label, path) in enumerate(tqdm(zip(image_features, image_labels, image_paths), total=len(image_paths))):\n",
    "    filename = os.path.basename(path)  # e.g., 'I31143_AD_axial_55.png'\n",
    "    img_id = filename.split('_')[0]             # 'I31143'\n",
    "\n",
    "    if img_id in text_lookup:\n",
    "        text_feat = text_lookup[img_id]\n",
    "        fused = np.concatenate([img_feat, text_feat])  # shape (640,)\n",
    "        fused_features.append(fused)\n",
    "        fused_labels.append(label)\n",
    "        matched_count += 1\n",
    "\n",
    "print(f\" Matched samples: {matched_count}\")\n",
    "\n",
    "# Convert to arrays and save\n",
    "fused_features = np.array(fused_features)\n",
    "fused_labels = np.array(fused_labels)\n",
    "\n",
    "np.save(\"val_fused_features_clean.npy\", fused_features)\n",
    "np.save(\"val_fused_labels_clean.npy\", fused_labels)\n",
    "\n",
    "print(\" Final val fused shape:\", fused_features.shape)\n",
    "print(\" Labels shape:\", fused_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ac337",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "X = np.load(\"train_fused_features_clean.npy\")\n",
    "y = np.load(\"train_fused_labels_clean.npy\")\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train_fused_embeddings_with_labels.csv\", index=False)\n",
    "print(\" Saved: train_fused_embeddings_with_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c2db0",
   "metadata": {},
   "source": [
    "savinf into csv the validation fused sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ef189",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "X = np.load(\"val_fused_features_clean.npy\")\n",
    "y = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"val_fused_embeddings_with_labels.csv\", index=False)\n",
    "print(\" Saved: val_fused_embeddings_with_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50158e89",
   "metadata": {},
   "source": [
    "saving image and textual data separately for training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c40f4",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load fused embeddings\n",
    "fused = np.load(\"train_fused_features_clean.npy\")  # (84755, 640)\n",
    "\n",
    "# Split features\n",
    "image_features = fused[:, :256]   # CNN-based\n",
    "text_features  = fused[:, 256:]   # Sentence-transformer-based\n",
    "\n",
    "# Save separately\n",
    "np.save(\"train_image_features_only.npy\", image_features)\n",
    "np.save(\"train_text_features_only.npy\", text_features)\n",
    "\n",
    "print(\" Saved:\")\n",
    "print(\"  train_image_features_only.npy (shape:\", image_features.shape, \")\")\n",
    "print(\"  train_text_features_only.npy  (shape:\", text_features.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136a005",
   "metadata": {},
   "source": [
    "saving image and textual data separately for validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e17f8",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load fused embeddings\n",
    "fused = np.load(\"val_fused_features_clean.npy\")  # (84755, 640)\n",
    "\n",
    "# Split features\n",
    "image_features = fused[:, :256]   # CNN-based\n",
    "text_features  = fused[:, 256:]   # Sentence-transformer-based\n",
    "\n",
    "# Save separately\n",
    "np.save(\"val_image_features_only.npy\", image_features)\n",
    "np.save(\"val_text_features_only.npy\", text_features)\n",
    "\n",
    "print(\" Saved:\")\n",
    "print(\"  val_image_features_only.npy (shape:\", image_features.shape, \")\")\n",
    "print(\"  val_text_features_only.npy  (shape:\", text_features.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa45749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc33f960",
   "metadata": {},
   "source": [
    "the code below is added just to save the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d179aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24f29456",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# üîπ Step 1: Load training set fused features and labels\n",
    "X_train = np.load(\"train_fused_features_clean.npy\")   # Shape: (84755, 640)\n",
    "y_train = np.load(\"train_fused_labels_clean.npy\")     # Shape: (84755,)\n",
    "\n",
    "# Step 2: Load validation set fused features and labels\n",
    "X_val = np.load(\"val_fused_features_clean.npy\")\n",
    "y_val = np.load(\"val_fused_labels_clean.npy\")\n",
    "\n",
    "# üîπ Step 2: Train/test split (not used for testing file)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# üîπ Step 3: Define and train MLP (no need of fitting again in test set)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256, 64),   # You can adjust the architecture\n",
    "    activation='relu',\n",
    "    learning_rate_init=0.0006,\n",
    "    solver='adam',\n",
    "    max_iter=50,          # Increase to 100‚Äì300 for better results if time allows\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# üîπ Step 4: Predict and evaluate\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n MLP Accuracy: {acc:.4f}\")\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"AD\", \"CN\", \"MCI\"]))\n",
    "\n",
    "# üîπ Step 5: Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=[\"AD\", \"CN\", \"MCI\"], yticklabels=[\"AD\", \"CN\", \"MCI\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\" Confusion Matrix - MLP (Fused Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd979ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# --------------------- Define MLP Model ---------------------\n",
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 3)  # 3 classes: AD, CN, MCI\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87494b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved:\n",
      " - train_text_features_only.npy (2294, 512)\n",
      " - train_text_labels_only.npy (2294,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/Users/fatimatuzzahra/Downloads/ADNI1_Complete_1Yr_1.5T_12_20_2024.csv\")\n",
    "\n",
    "# üîπ Step 1: Keep label and encode it numerically\n",
    "label_map = {'AD': 0, 'CN': 1, 'MCI': 2}\n",
    "df = df[df['Group'].isin(label_map)]  # filter out unknowns\n",
    "df['label'] = df['Group'].map(label_map)\n",
    "\n",
    "# üîπ Step 2: Prepare text input (excluding label columns)\n",
    "text_input_cols = [col for col in df.columns if col not in ['Group', 'label', 'Downloaded', 'Modality', 'Type', 'Format']]\n",
    "df_text = df[text_input_cols]\n",
    "\n",
    "# üîπ Optional: Keep Image IDs if you want to track\n",
    "image_ids = df['Image Data ID'].values\n",
    "\n",
    "# üîπ Step 3: Combine row text into string per patient\n",
    "texts = df_text.astype(str).agg(\" \".join, axis=1).tolist()\n",
    "\n",
    "# üîπ Step 4: Tokenize with CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "# üîπ Step 5: Generate text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens).cpu().numpy()\n",
    "\n",
    "# üîπ Step 6: Save embeddings and labels (ready for training!)\n",
    "np.save(\"train_text_features_only.npy\", text_embeddings)\n",
    "np.save(\"train_text_labels_only.npy\", df['label'].values)\n",
    "\n",
    "# Optional: save a CSV for review\n",
    "df_out = pd.DataFrame(text_embeddings)\n",
    "df_out['label'] = df['label'].values\n",
    "df_out.to_csv(\"train_text_features_with_labels.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved:\")\n",
    "print(\" - train_text_features_only.npy\", text_embeddings.shape)\n",
    "print(\" - train_text_labels_only.npy\", df['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2272d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the image split path files\n",
    "train_paths = np.load(\"train_image_paths_axial.npy\", allow_pickle=True)\n",
    "val_paths = np.load(\"val_image_paths_axial.npy\", allow_pickle=True)\n",
    "test_paths = np.load(\"test_image_paths_axial.npy\", allow_pickle=True)\n",
    "\n",
    "def extract_ids(paths):\n",
    "    return set(os.path.basename(p).split(\"_\")[0] for p in paths)\n",
    "\n",
    "train_ids = extract_ids(train_paths)\n",
    "val_ids = extract_ids(val_paths)\n",
    "test_ids = extract_ids(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "650a83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your text embeddings and image IDs:\n",
    "\n",
    "text_embeddings = np.load(\"train_text_features_only.npy\")\n",
    "text_labels = np.load(\"train_text_labels_only.npy\")\n",
    "text_ids = np.load(\"text_image_ids.npy\", allow_pickle=True)  # assuming you saved these earlier\n",
    "text_ids = df['Image Data ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d90a00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now match and split\n",
    "\n",
    "train_feats, val_feats, test_feats = [], [], []\n",
    "train_labs, val_labs, test_labs = [], [], []\n",
    "\n",
    "for emb, label, tid in zip(text_embeddings, text_labels, text_ids):\n",
    "    tid_str = str(tid)\n",
    "    if tid_str in train_ids:\n",
    "        train_feats.append(emb)\n",
    "        train_labs.append(label)\n",
    "    elif tid_str in val_ids:\n",
    "        val_feats.append(emb)\n",
    "        val_labs.append(label)\n",
    "    elif tid_str in test_ids:\n",
    "        test_feats.append(emb)\n",
    "        test_labs.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55ddc1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1541, 512)\n",
      "Val: (153, 512)\n",
      "Test: (226, 512)\n"
     ]
    }
   ],
   "source": [
    "# Convert and save if needed (for cross-checking\n",
    "\n",
    "train_feats, val_feats, test_feats = map(np.array, [train_feats, val_feats, test_feats])\n",
    "train_labs, val_labs, test_labs = map(np.array, [train_labs, val_labs, test_labs])\n",
    "\n",
    "np.save(\"text_train_features.npy\", train_feats)\n",
    "np.save(\"text_train_labels.npy\", train_labs)\n",
    "\n",
    "np.save(\"text_val_features.npy\", val_feats)\n",
    "np.save(\"text_val_labels.npy\", val_labs)\n",
    "\n",
    "np.save(\"text_test_features.npy\", test_feats)\n",
    "np.save(\"text_test_labels.npy\", test_labs)\n",
    "\n",
    "print(\"Train:\", train_feats.shape)\n",
    "print(\"Val:\", val_feats.shape)\n",
    "print(\"Test:\", test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1daffc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "# --------------------- Load Data ---------------------\n",
    "X_train = np.load(\"train_text_features_only.npy\")\n",
    "y_train = np.load(\"train_text_labels_only.npy\")\n",
    "\n",
    "X_val = np.load(\"text_val_features.npy\")\n",
    "y_val = np.load(\"text_val_labels.npy\")\n",
    "\n",
    "# --------------------- Normalize ---------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# --------------------- SMOTE Oversampling ---------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# --------------------- Tensor Conversion ---------------------\n",
    "X_train_tensor = torch.tensor(X_train_bal, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_bal, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "# --------------------- Training Setup ---------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MLPWithDropout().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "patience = 5  # You can tune this\n",
    "early_stop = False\n",
    "\n",
    "best_model_state = None  # To save the best model\n",
    "# --------------------- Train Loop ---------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # ---------------- Validate ----------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # -------- Early Stopping Check --------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"‚õî Early stopping at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------- Evaluate on Training Set ---------------------\n",
    "model.eval()\n",
    "y_train_preds = []\n",
    "y_train_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        y_train_preds.extend(preds.cpu().numpy())\n",
    "        y_train_true.extend(yb.numpy())\n",
    "\n",
    "train_acc = accuracy_score(y_train_true, y_train_preds)\n",
    "print(f\"üìä Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# --------------------- Evaluate on Validation Set ---------------------\n",
    "y_val_preds = []\n",
    "y_val_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        y_val_preds.extend(preds.cpu().numpy())\n",
    "        y_val_true.extend(yb.numpy())\n",
    "\n",
    "val_acc = accuracy_score(y_val_true, y_val_preds)\n",
    "print(f\"‚úÖ Validation Accuracy: {val_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val_true, y_val_preds, target_names=[\"AD\", \"CN\", \"MCI\"]))\n",
    "\n",
    "# --------------------- Confusion Matrix ---------------------\n",
    "cm = confusion_matrix(y_val_true, y_val_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Purples\", xticklabels=[\"AD\", \"CN\", \"MCI\"], yticklabels=[\"AD\", \"CN\", \"MCI\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - MLP (Validation Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82e51e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 0.4314\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.23      0.24      0.23        34\n",
      "          CN       0.42      0.38      0.40        47\n",
      "         MCI       0.53      0.56      0.54        72\n",
      "\n",
      "    accuracy                           0.43       153\n",
      "   macro avg       0.39      0.39      0.39       153\n",
      "weighted avg       0.43      0.43      0.43       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming y_test and y_pred are already defined from your previous evaluation\n",
    "\n",
    "# Define class labels (you can update these if your label encoding is different)\n",
    "class_names = ['AD', 'CN', 'MCI']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val_true, y_val_preds)\n",
    "print(f\"\\n Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_val_true, y_val_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac21360",
   "metadata": {},
   "source": [
    "\t.   ‚úÖ Replace np.concatenate() fusion with a trainable layer\n",
    "\t‚Ä¢\t‚úÖ Try BioCLIP or medical image-pretrained models\n",
    "\t‚Ä¢\t‚úÖ Improve tabular/text embedding via BioBERT or using key columns only\n",
    "\t‚Ä¢\t‚úÖ Regularize fusion with modality dropout\n",
    "\t‚Ä¢\t‚úÖ (Optional) Try late fusion/ensemble as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c7927",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
